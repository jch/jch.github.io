<!DOCTYPE html>
<html>
<head>
<meta content='width=device-width, initial-scale=1.0' name='viewport'>
<title>
Planning a Big Data Migration
</title>
<link href="../../../../assets/application-1b6070a43c3e8d37df5eb4ad2c23b344.css" media="all" rel="stylesheet" type="text/css" />
<script type="text/javascript">
  var _gaq = _gaq || [];
  _gaq.push(['_setAccount', 'UA-2747647-1']);
  _gaq.push(['_setSiteSpeedSampleRate', 100]);
  _gaq.push(['_trackPageview']);

  (function() {
    var ga = document.createElement('script'); ga.type = 'text/javascript'; ga.async = true;
    ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(ga, s);
  })();
</script>

</head>
<body class='container-fluid'>
<div class='full-width' id='wcc'>
<div id='header'>
<a href="../../../../index.html">whatcodecraves.com</a>
</div>

<div id='main'>
<div id="carbonads-container">
  <div class="carbonad">
    <div id="azcarbon"></div>
      <script type="text/javascript">
        var z = document.createElement("script");
        z.type = "text/javascript";
        z.async = true;
        z.src = "http://engine.carbonads.com/z/17311/azcarbon_2_1_0_VERT";
        var s = document.getElementsByTagName("script")[0];
        s.parentNode.insertBefore(z, s);
      </script>
    </div>
  </div>
<div class='post'>
<h1>Planning a Big Data Migration</h1>
<a href="http://news.ycombinator.com/submit" class="hn-share-button">Vote on HN</a>
<a href="https://twitter.com/share" class="twitter-share-button" data_via="whatcodecraves">Tweet</a>
<div class='g-plus' data-action='share' data-annontation='bubble'></div>
<p>It doesn't matter if data is being migrated from SQL to NoSQL, from flat
files to key-value store, or from XML to an object database, or every
permutation of any data store to any other data store. What stays constant is
the fact that data migrations are scary and painful. Without the right strategy, a big
data migration will leave you with inconsistent data, strange errors, and very
angry users. Read on for a data migration checklist that'll save you days of
headache.</p><p>It doesn't matter if data is being migrated from SQL to NoSQL, from flat
files to key-value store, or from XML to an object database, or every
permutation of any data store to any other data store. What stays constant is
the fact that data migrations are scary and painful. Without the right strategy, a big
data migration will leave you with inconsistent data, strange errors, and very
angry users. Read on for a data migration checklist that'll save you days of
headache.</p><h2>Backup</h2><p>Before even considering a massive destructive mutation of your data, you
should have working backups. The keyword is <strong>"working"</strong>. Take production
dumps of your data, and make sure you can load the same data on a cloned
environment. If anything goes wrong when migration day comes along, these
backups will be your last line of defense. Backups are also useful for doing
practice runs of a migration.</p><h2>Logging</h2><p>Create a logger that logs to a separate place from your application logs
that's specific for the migration. When the migration is running, the logger
should warn on strange data and error on exceptional cases. To keep the log
useful, it's important not to flood it with debugging information. Log only
the most important details needed for troubleshooting problems: a timestamp,
an id reference to the failing record, and a brief description of the failure
reason.</p><h2>Atomicity</h2><p>Regardless of whether the destination data store supports transactions or not,
the migration should always define an invariant for when a record is
successfully imported. If this invariant is broken, then whatever has been
done to break the invariant should be undone so that your data isn't in some
zombie half consistent state.</p><h2>Idempotence</h2><p>Not strictly the definition of
<a href="http://en.wikipedia.org/wiki/Idempotence">idempotence</a>, but similar to
maintaining consistency, your code should be able to handle re-migrating the
same data. If the migration crashes halfway, having this property allows you
restart and import again without worrying about weird state issues.</p><h2>Batch Processing</h2><p>Having atomicity and idempotence lets your migration be split up into smaller
migrations. Instead of migrating a million records in an all-or-nothing
migration and crossing your fingers, you can split them up into small 500
record batches. If any single batch fails, you can redo just that single
batch, rather than redo the entire migration. This also allows you to balance
the migration across more resources like multiprocessors, different servers,
and different slave databases.</p><h2>Validation</h2><p>After a migration is complete, it's important to be able to validate that
everything is still working. This means running your test suite, your
integration tests, and also logging in as existing users and clicking around.</p><h2>Live Migrations</h2><p>Running a migration with scheduled downtime is hard enough as it is, but in
certain applications, a big chunk of downtime is unacceptable. If this is the
case, then it's critical to add bookkeeping code that tracks which records has
been migrated and which haven't. This allows you to query and incrementally
upgrade parts of your system while co-existing with old data and old code.</p><h2>Plan Ahead</h2><p>Data migrations will always be a chore. But with the right strategy, at least
it'll be one that can be finished, rather than something that drags along and
repeatedly slows down your whole team.</p>
</div>
<script src="../../../../assets/post-6db30d9cd8415ddc540433d54a06699e.js" type="text/javascript"></script>
<script>
  //<![CDATA[
    hljs.initHighlightingOnLoad();
  //]]>
</script>
<script type='text/javascript'>
  // HackerNews
  (function(d, t) {
    var g = d.createElement(t),
        s = d.getElementsByTagName(t)[0];
    g.src = '//hnbutton.appspot.com/static/hn.js';
    s.parentNode.insertBefore(g, s);
  }(document, 'script'));

  // Twitter
  !function(d,s,id){var js,fjs=d.getElementsByTagName(s)[0];if(!d.getElementById(id)){js=d.createElement(s);js.id=id;js.src="//platform.twitter.com/widgets.js";fjs.parentNode.insertBefore(js,fjs);}}(document,"script","twitter-wjs");

  // Google Plus
  (function() {
    var po = document.createElement('script'); po.type = 'text/javascript'; po.async = true;
    po.src = 'https://apis.google.com/js/plusone.js';
    var s = document.getElementsByTagName('script')[0]; s.parentNode.insertBefore(po, s);
  })();
</script>
<div id="disqus_thread"></div>
<script type="text/javascript">
  var disqus_shortname  = 'whatcodecraves';
  var disqus_title      = 'Planning a Big Data Migration';
  var disqus_identifier = '/articles/2011/12/01/planning-a-big-data-migration';


  /* * * DON'T EDIT BELOW THIS LINE * * */
  (function() {
    var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
    dsq.src = 'http://' + disqus_shortname + '.disqus.com/embed.js';
    (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
  })();
</script>
<noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>


</div>
<div id='footer'>
<p>
Copyright Jerry Cheung 2007 - 2014 &nbsp;&nbsp;
<a href="https://github.com/jch/whatcoderaves.com">Source for this blog</a>
</p>
</div>
</div>
</body>
</html>
